{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQ2KGyLJ99Yi"
   },
   "source": [
    "# 랭체인 사전 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkPbwp6-ploX"
   },
   "outputs": [],
   "source": [
    "# 패키지 설치\n",
    "!pip install langchain==0.0.181\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Gf8Rfw2MiG_"
   },
   "outputs": [],
   "source": [
    "# 환경변수 준비\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<OpenAI_API의 API키>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPJ2lQW0-J3a"
   },
   "source": [
    "# 텍스트 생성 모델의 LLM 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBYKjuiZBAXn"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# LLM 준비\n",
    "llm = OpenAI(\n",
    "    model_name=\"text-davinci-003\",  # 모델 ID\n",
    "    temperature=0  # 무작위성\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNWwnfTBrhNJ"
   },
   "outputs": [],
   "source": [
    "# LLM 호출\n",
    "result = llm(\"고양이 울음소리는?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZ-l9ky6x3I8"
   },
   "outputs": [],
   "source": [
    "# 고급 LLM 호출\n",
    "result = llm.generate([\"고양이 울음소리는?\", \"까마귀 울음소리는?\"])\n",
    "\n",
    "# 출력 텍스트\n",
    "print(\"response0:\", result.generations[0][0].text)\n",
    "print(\"response1:\", result.generations[1][0].text)\n",
    "\n",
    "# 사용한 토큰 개수\n",
    "print(\"llm_output:\", result.llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cm1RHdPjrhX2"
   },
   "source": [
    "# 채팅 모델의 LLM 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQOSHZfzkvlY"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# LLM 준비\n",
    "chat_llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",  # 모델 ID\n",
    "    temperature=0  # 무작위성\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5vQ7muHrxg4"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "# LLM 호출\n",
    "messages = [\n",
    "    HumanMessage(content=\"고양이 울음소리는?\")\n",
    "]\n",
    "result = chat_llm(messages)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOUbE8-XxGkU"
   },
   "outputs": [],
   "source": [
    "# 고급 LLM 호출\n",
    "messages_list = [\n",
    "    [HumanMessage(content=\"고양이 울음소리는?\")],\n",
    "    [HumanMessage(content=\"까마귀 울음소리는?\")]\n",
    "]\n",
    "result = chat_llm.generate(messages_list)\n",
    "\n",
    "# 출력 텍스트\n",
    "print(\"response0:\", result.generations[0][0].text)\n",
    "print(\"response1:\", result.generations[1][0].text)\n",
    "\n",
    "# 사용한 토큰 개수\n",
    "print(\"llm_output:\", result.llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1PIjv-pllHE"
   },
   "source": [
    "# 캐시 활성화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSNGZIhOlRTq"
   },
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "# 캐시 활성화\n",
    "langchain.llm_cache = InMemoryCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vpl3by-UlkEq"
   },
   "outputs": [],
   "source": [
    "# 첫 번째 LLM 호출\n",
    "llm.generate([\"하늘의 색깔은?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kiAWmyCmlMmq"
   },
   "outputs": [],
   "source": [
    "# 2번째 이후 LLM 호출\n",
    "llm.generate([\"하늘의 색깔은?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVGJochJoDRI"
   },
   "source": [
    "# 특정 LLM의 캐시 비활성화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ciUncDwyoE3u"
   },
   "outputs": [],
   "source": [
    "# 특정 LLM에 대한 메모리 캐시 비활성화\n",
    "llm = OpenAI(cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3gzGXoPjo6uE"
   },
   "outputs": [],
   "source": [
    "# LLM 호출\n",
    "llm.generate([\"하늘의 색깔은?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRhplZMApPHY"
   },
   "source": [
    "# 캐시 비활성화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1Sz4rNApK11"
   },
   "outputs": [],
   "source": [
    "# 캐시 비활성화\n",
    "langchain.llm_cache = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7zHImzmriiV"
   },
   "outputs": [],
   "source": [
    "#LLM 호출\n",
    "llm.generate([\"하늘의 색깔은?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOkBvnG0o7OV"
   },
   "source": [
    "# LLM의 비동기 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wW6szBSZo87d"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# 동기화 처리로 10번 호출하는 함수\n",
    "def generate_serially():\n",
    "    llm = OpenAI(temperature=0.9)\n",
    "    for _ in range(10):\n",
    "        resp = llm.generate([\"안녕하세요!\"])\n",
    "        print(resp.generations[0][0].text)\n",
    "\n",
    "\n",
    "# 시간 측정 시작\n",
    "s = time.perf_counter()\n",
    "\n",
    "# 동기화 처리로 10번 호출\n",
    "generate_serially()\n",
    "\n",
    "# 시간 측정 완료\n",
    "elapsed = time.perf_counter() - s\n",
    "print(f\"{elapsed:0.2f} 초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rP9GZ6NTpBAZ"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# 이벤트 루프를 중첩하는 설정\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 비동기 처리로 한 번만 호출하는 함수\n",
    "async def async_generate(llm):\n",
    "    resp = await llm.agenerate([\"안녕하세요!\"])\n",
    "    print(resp.generations[0][0].text)\n",
    "\n",
    "# 비동기 처리로 10회 호출하는 함수\n",
    "async def generate_concurrently():\n",
    "    llm = OpenAI(temperature=0.9)\n",
    "    tasks = [async_generate(llm) for _ in range(10)]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "# 시간 측정 시작\n",
    "s = time.perf_counter()\n",
    "\n",
    "# 비동기 처리로 10회 호출\n",
    "asyncio.run(generate_concurrently())\n",
    "\n",
    "# 시간 측정 완료\n",
    "elapsed = time.perf_counter() - s\n",
    "print(f\"{elapsed:0.2f} 초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WnL2UytsNTz"
   },
   "source": [
    "# LLM 스트리밍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "IH-PAwYssMuE"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# 스트리밍 방식으로 출력할 LLM을 준비\n",
    "llm = OpenAI(\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    verbose=True,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# LLM 호출\n",
    "resp = llm(\"즐거운 ChatGPT 생활을 가사로 만들어 주세요.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMvQq897BKCljJuQeZyoJr1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
